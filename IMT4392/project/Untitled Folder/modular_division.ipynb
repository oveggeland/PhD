{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623ef857",
   "metadata": {},
   "source": [
    "# Reproduction of grokking on modular addition problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685b927",
   "metadata": {},
   "source": [
    "## Step 1: Produce dataset\n",
    "I start by producing a simple algorithmic dataset, similar to the one in the core paper. The network will be trained to evaluate an expression of the type (a + b) mod p, where a and b are numbered inputs and p is a prime number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db4bc4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe0ba583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset of shape (9312, 3)\n"
     ]
    }
   ],
   "source": [
    "# Dataset params\n",
    "p = 97\n",
    "\n",
    "# Create division dataset\n",
    "dataset = np.zeros((p*(p-1), 3))\n",
    "dataset[:, 0] = np.array([[i]*p for i in range(p-1)]).reshape(-1)\n",
    "dataset[:, 1] = np.kron(np.ones(p), np.arange(1, p))\n",
    "dataset[:, 2] = (dataset[:, 0] / dataset[:, 1]) % p\n",
    "\n",
    "print(\"Created dataset of shape\", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53375582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (931, 3)\n",
      "Val set shape: (8381, 3)\n"
     ]
    }
   ],
   "source": [
    "# Divide dataset according to a fraction\n",
    "train_ratio = 0.1\n",
    "train_set, val_set = train_test_split(dataset, test_size=(1-train_ratio))\n",
    "\n",
    "X_train = torch.Tensor(train_set[:, :2])\n",
    "Y_train = torch.Tensor(train_set[:, 2])\n",
    "X_val = torch.Tensor(val_set[:, :2])\n",
    "Y_val = torch.Tensor(val_set[:, 2])\n",
    "\n",
    "print(\"Train set shape:\", train_set.shape)\n",
    "print(\"Val set shape:\", val_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7c0e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2822 6587\n"
     ]
    }
   ],
   "source": [
    "def gen_train_test(frac_train, num, seed=0):\n",
    "    # Generate train and test split\n",
    "    pairs = [(i, j, num) for i in range(num) for j in range(num)]\n",
    "    random.seed(seed)\n",
    "    random.shuffle(pairs)\n",
    "    div = int(frac_train*len(pairs))\n",
    "    return pairs[:div], pairs[div:]\n",
    "\n",
    "train, test = gen_train_test(frac_train, p, seed)\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2dbf78",
   "metadata": {},
   "source": [
    "## Step 2: Define model (2L decoder only transformer)\n",
    "Using NEEL NANDAS (INTERPRETABILITY) as inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fb21e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper class to get access to intermediate activations (inspired by Garcon)\n",
    "# It's a dummy module that is the identity function by default\n",
    "# I can wrap any intermediate activation in a HookPoint and get a convenient \n",
    "# way to add PyTorch hooks\n",
    "class HookPoint(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fwd_hooks = []\n",
    "        self.bwd_hooks = []\n",
    "    \n",
    "    def give_name(self, name):\n",
    "        # Called by the model at initialisation\n",
    "        self.name = name\n",
    "    \n",
    "    def add_hook(self, hook, dir='fwd'):\n",
    "        # Hook format is fn(activation, hook_name)\n",
    "        # Change it into PyTorch hook format (this includes input and output, \n",
    "        # which are the same for a HookPoint)\n",
    "        def full_hook(module, module_input, module_output):\n",
    "            return hook(module_output, name=self.name)\n",
    "        if dir=='fwd':\n",
    "            handle = self.register_forward_hook(full_hook)\n",
    "            self.fwd_hooks.append(handle)\n",
    "        elif dir=='bwd':\n",
    "            handle = self.register_backward_hook(full_hook)\n",
    "            self.bwd_hooks.append(handle)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid direction {dir}\")\n",
    "    \n",
    "    def remove_hooks(self, dir='fwd'):\n",
    "        if (dir=='fwd') or (dir=='both'):\n",
    "            for hook in self.fwd_hooks:\n",
    "                hook.remove()\n",
    "            self.fwd_hooks = []\n",
    "        if (dir=='bwd') or (dir=='both'):\n",
    "            for hook in self.bwd_hooks:\n",
    "                hook.remove()\n",
    "            self.bwd_hooks = []\n",
    "        if dir not in ['fwd', 'bwd', 'both']:\n",
    "            raise ValueError(f\"Invalid direction {dir}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e0d2111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture\n",
    "# I defined my own transformer from scratch so I'd fully understand each component \n",
    "# - I expect this wasn't necessary or particularly important, and a bunch of this \n",
    "# replicates existing PyTorch functionality\n",
    "\n",
    "# Embed & Unembed\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.einsum('dbp -> bpd', self.W_E[:, x])\n",
    "\n",
    "class Unembed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_U = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_vocab))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (x @ self.W_U)\n",
    "\n",
    "# Positional Embeddings\n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, max_ctx, d_model):\n",
    "        super().__init__()\n",
    "        self.W_pos = nn.Parameter(torch.randn(max_ctx, d_model)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x+self.W_pos[:x.shape[-2]]\n",
    "\n",
    "# LayerNorm\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, epsilon = 1e-4, model=[None]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.w_ln = nn.Parameter(torch.ones(d_model))\n",
    "        self.b_ln = nn.Parameter(torch.zeros(d_model))\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.model[0].use_ln:\n",
    "            x = x - x.mean(axis=-1)[..., None]\n",
    "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
    "            x = x * self.w_ln\n",
    "            x = x + self.b_ln\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_head, n_ctx, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_O = nn.Parameter(torch.randn(d_model, d_head * num_heads)/np.sqrt(d_model))\n",
    "        self.register_buffer('mask', torch.tril(torch.ones((n_ctx, n_ctx))))\n",
    "        self.d_head = d_head\n",
    "        self.hook_k = HookPoint()\n",
    "        self.hook_q = HookPoint()\n",
    "        self.hook_v = HookPoint()\n",
    "        self.hook_z = HookPoint()\n",
    "        self.hook_attn = HookPoint()\n",
    "        self.hook_attn_pre = HookPoint()\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.hook_k(torch.einsum('ihd,bpd->biph', self.W_K, x))\n",
    "        q = self.hook_q(torch.einsum('ihd,bpd->biph', self.W_Q, x))\n",
    "        v = self.hook_v(torch.einsum('ihd,bpd->biph', self.W_V, x))\n",
    "        attn_scores_pre = torch.einsum('biph,biqh->biqp', k, q)\n",
    "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (1 - self.mask[:x.shape[-2], :x.shape[-2]])\n",
    "        attn_matrix = self.hook_attn(F.softmax(self.hook_attn_pre(attn_scores_masked/np.sqrt(self.d_head)), dim=-1))\n",
    "        z = self.hook_z(torch.einsum('biph,biqp->biqh', v, attn_matrix))\n",
    "        z_flat = einops.rearrange(z, 'b i q h -> b q (i h)')\n",
    "        out = torch.einsum('df,bqf->bqd', self.W_O, z_flat)\n",
    "        return out\n",
    "\n",
    "# MLP Layers\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_model)/np.sqrt(d_model))\n",
    "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
    "        self.W_out = nn.Parameter(torch.randn(d_model, d_mlp)/np.sqrt(d_model))\n",
    "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
    "        self.act_type = act_type\n",
    "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
    "        self.hook_pre = HookPoint()\n",
    "        self.hook_post = HookPoint()\n",
    "        assert act_type in ['ReLU', 'GeLU']\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hook_pre(torch.einsum('md,bpd->bpm', self.W_in, x) + self.b_in)\n",
    "        if self.act_type=='ReLU':\n",
    "            x = F.relu(x)\n",
    "        elif self.act_type=='GeLU':\n",
    "            x = F.gelu(x)\n",
    "        x = self.hook_post(x)\n",
    "        x = torch.einsum('dm,bpm->bpd', self.W_out, x) + self.b_out\n",
    "        return x\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
    "        self.attn = Attention(d_model, num_heads, d_head, n_ctx, model=self.model)\n",
    "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
    "        self.mlp = MLP(d_model, d_mlp, act_type, model=self.model)\n",
    "        self.hook_attn_out = HookPoint()\n",
    "        self.hook_mlp_out = HookPoint()\n",
    "        self.hook_resid_pre = HookPoint()\n",
    "        self.hook_resid_mid = HookPoint()\n",
    "        self.hook_resid_post = HookPoint()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hook_resid_mid(x + self.hook_attn_out(self.attn((self.hook_resid_pre(x)))))\n",
    "        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n",
    "        return x\n",
    "\n",
    "# Full transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_vocab, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, use_cache=False, use_ln=True):\n",
    "        super().__init__()\n",
    "        self.cache = {}\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        self.embed = Embed(d_vocab, d_model)\n",
    "        self.pos_embed = PosEmbed(n_ctx, d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]) for i in range(num_layers)])\n",
    "        # self.ln = LayerNorm(d_model, model=[self])\n",
    "        self.unembed = Unembed(d_vocab, d_model)\n",
    "        self.use_ln = use_ln\n",
    "\n",
    "        for name, module in self.named_modules():\n",
    "            if type(module)==HookPoint:\n",
    "                module.give_name(name)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        # x = self.ln(x)\n",
    "        x = self.unembed(x)\n",
    "        return x\n",
    "\n",
    "    def set_use_cache(self, use_cache):\n",
    "        self.use_cache = use_cache\n",
    "    \n",
    "    def hook_points(self):\n",
    "        return [module for name, module in self.named_modules() if 'hook' in name]\n",
    "\n",
    "    def remove_all_hooks(self):\n",
    "        for hp in self.hook_points():\n",
    "            hp.remove_hooks('fwd')\n",
    "            hp.remove_hooks('bwd')\n",
    "    \n",
    "    def cache_all(self, cache, incl_bwd=False):\n",
    "        # Caches all activations wrapped in a HookPoint\n",
    "        def save_hook(tensor, name):\n",
    "            cache[name] = tensor.detach()\n",
    "        def save_hook_back(tensor, name):\n",
    "            cache[name+'_grad'] = tensor[0].detach()\n",
    "        for hp in self.hook_points():\n",
    "            hp.add_hook(save_hook, 'fwd')\n",
    "            if incl_bwd:\n",
    "                hp.add_hook(save_hook_back, 'bwd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8144ac24",
   "metadata": {},
   "source": [
    "## Step 3: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d42a0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "weight_decay = 1.0\n",
    "d_model = 128\n",
    "num_epochs = 50000\n",
    "save_models = False\n",
    "save_every = 100\n",
    "stopping_thresh = -1\n",
    "seed = 0\n",
    "\n",
    "num_layers = 1\n",
    "batch_style = 'full'\n",
    "d_vocab = p+1\n",
    "n_ctx = 3\n",
    "d_mlp = 4*d_model\n",
    "num_heads = 4\n",
    "assert d_model % num_heads == 0, \"Error with parameters\"\n",
    "d_head = d_model//num_heads\n",
    "act_type = 'ReLU' #@param ['ReLU', 'GeLU']\n",
    "use_ln = False\n",
    "random_answers = np.random.randint(low=0, high=p, size=(p, p))\n",
    "fns_dict = {'add': lambda x,y:(x+y)%p, 'subtract': lambda x,y:(x-y)%p, 'x2xyy2':lambda x,y:(x**2+x*y+y**2)%p, 'rand':lambda x,y:random_answers[x][y]}\n",
    "fn = fns_dict[fn_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaaa01a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for model training\n",
    "\n",
    "def full_loss(model, data, device):\n",
    "    # Take the final position only\n",
    "    logits = model(data)[:, -1]\n",
    "    labels = torch.tensor([fn(i, j) for i, j, _ in data]).to(device)\n",
    "    return cross_entropy_high_precision(logits, labels)\n",
    "\n",
    "def cross_entropy_high_precision(logits, labels):\n",
    "    # Shapes: batch x vocab, batch\n",
    "    # Cast logits to float64 because log_softmax has a float32 underflow on overly \n",
    "    # confident data and can only return multiples of 1.2e-7 (the smallest float x\n",
    "    # such that 1+x is different from 1 in float32). This leads to loss spikes \n",
    "    # and dodgy gradients\n",
    "    logprobs = F.log_softmax(logits.to(torch.float64), dim=-1)\n",
    "    prediction_logprobs = torch.gather(logprobs, index=labels[:, None], dim=-1)\n",
    "    loss = -torch.mean(prediction_logprobs)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name grok_1666909175\n",
      "0_1.5599_1.5586\n",
      "100_0.8707_2.0534\n",
      "200_-3.6560_2.8766\n",
      "300_-4.7438_2.9005\n",
      "400_-5.8725_2.9522\n",
      "500_-6.9757_3.0070\n",
      "600_-8.0646_3.0618\n",
      "700_-9.1375_3.1154\n",
      "800_-10.1946_3.1674\n",
      "900_-11.2324_3.2173\n",
      "1000_-12.2308_3.2637\n",
      "1100_-13.1518_3.3048\n",
      "1200_-13.9343_3.3371\n",
      "1300_-14.5117_3.3586\n",
      "1400_-14.8559_3.3683\n",
      "1500_-15.0048_3.3680\n",
      "1600_-15.0472_3.3625\n",
      "1700_-15.0551_3.3550\n",
      "1800_-15.0572_3.3471\n",
      "1900_-15.0595_3.3393\n",
      "2000_-15.0614_3.3312\n",
      "2100_-15.0650_3.3233\n",
      "2200_-15.0688_3.3152\n",
      "2300_-15.0732_3.3072\n",
      "2400_-15.0772_3.2991\n",
      "2500_-15.0824_3.2906\n",
      "2600_-15.0872_3.2819\n",
      "2700_-15.0914_3.2729\n",
      "2800_-15.0961_3.2638\n",
      "2900_-15.1010_3.2549\n",
      "3000_-15.1056_3.2458\n",
      "3100_-15.1096_3.2363\n",
      "3200_-15.1144_3.2264\n",
      "3300_-15.1184_3.2161\n",
      "3400_-15.1232_3.2055\n",
      "3500_-15.1267_3.1945\n",
      "3600_-15.1314_3.1833\n",
      "3700_-15.1361_3.1719\n",
      "3800_-15.1408_3.1596\n",
      "3900_-15.1454_3.1470\n",
      "4000_-15.1502_3.1337\n",
      "4100_-15.1548_3.1198\n",
      "4200_-15.1592_3.1052\n",
      "4300_-15.1647_3.0900\n",
      "4400_-15.1694_3.0739\n",
      "4500_-15.1743_3.0569\n",
      "4600_-15.1777_3.0391\n",
      "4700_-15.1845_3.0203\n",
      "4800_-15.1895_3.0005\n",
      "4900_-15.1957_2.9787\n",
      "5000_-15.2007_2.9556\n",
      "5100_-15.2075_2.9316\n",
      "5200_-15.2121_2.9058\n",
      "5300_-15.2196_2.8779\n",
      "5400_-15.2271_2.8478\n",
      "5500_-15.2341_2.8154\n",
      "5600_-15.2406_2.7808\n",
      "5700_-15.2480_2.7438\n",
      "5800_-15.2576_2.7038\n",
      "5900_-15.2664_2.6597\n",
      "6000_-15.2774_2.6103\n",
      "6100_-15.2885_2.5546\n"
     ]
    }
   ],
   "source": [
    "# Init model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Transformer(num_layers=num_layers, d_vocab=d_vocab, d_model=d_model, d_mlp=d_mlp, d_head=d_head, num_heads=num_heads, n_ctx=n_ctx, act_type=act_type, use_cache=False, use_ln=use_ln)\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizing process\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.98))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step/10, 1))\n",
    "\n",
    "# Run formalities\n",
    "run_name = f\"grok_{int(time.time())}\"\n",
    "print(f'Run name {run_name}')\n",
    "if save_models:\n",
    "    os.mkdir(root/run_name)\n",
    "    save_dict = {'model':model.state_dict(), 'train_data':train, 'test_data':test}\n",
    "    torch.save(save_dict, root/run_name/'init.pth')\n",
    "    \n",
    "# Allocate lists for loss storage\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Train over several epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Calculate train and test loss\n",
    "    train_loss = full_loss(model, train, device)\n",
    "    train_losses.append(train_loss.item())\n",
    "    \n",
    "    test_loss = full_loss(model, test, device)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # Print status\n",
    "    if epoch%100 == 0: print(f\"{epoch}_{np.log(train_loss.item()):.4f}_{np.log(test_loss.item()):.4f}\")#_{train_acc.item():.4f}_{test_acc.item():.4f}\")\n",
    "    \n",
    "    # Calc gradients and perform backprop\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if test_loss.item() < stopping_thresh:\n",
    "        break\n",
    "        \n",
    "    if (save_models) and (epoch%save_every == 0):\n",
    "        if test_loss.item() < stopping_thresh:\n",
    "            break\n",
    "        save_dict = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(save_dict, root/run_name/f\"{epoch}.pth\")\n",
    "        print(f\"Saved model to {root/run_name/f'{epoch}.pth'}\")\n",
    "        \n",
    "if not save_models:\n",
    "    os.mkdir(root/run_name)\n",
    "save_dict = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "    'train_loss': train_loss,\n",
    "    'test_loss': test_loss,\n",
    "    'train_losses': train_losses,\n",
    "    'test_losses': test_losses,\n",
    "    'epoch': epoch,\n",
    "}\n",
    "\n",
    "torch.save(save_dict, root/run_name/f\"final.pth\")\n",
    "print(f\"Saved model to {root/run_name/f'final.pth'}\")\n",
    "lines([train_losses, test_losses], labels=['train', 'test'], log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e669cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
