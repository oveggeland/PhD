{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623ef857",
   "metadata": {},
   "source": [
    "# Reproduction of grokking on modular addition problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685b927",
   "metadata": {},
   "source": [
    "## Step 1: Produce dataset\n",
    "I start by producing a simple algorithmic dataset, similar to the one in the core paper. The network will be trained to evaluate an expression of the type (a + b) mod p, where a and b are numbered inputs and p is a prime number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db4bc4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import einops\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from notebook import notebookapp\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 0\n",
    "local_host = True if list(notebookapp.list_running_servers())[0]['hostname'] == 'localhost' else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0ba583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset params\n",
    "p = 97\n",
    "frac_train = 0.3\n",
    "\n",
    "# Create a randomly shuffled list of all possible combinations of input numbers: (i+j) % p = ?\n",
    "pairs = [(i, j, p) for i in range(p) for j in range(p)]\n",
    "random.seed(seed)\n",
    "random.shuffle(pairs)\n",
    "\n",
    "# Divide list into train and validation sets\n",
    "div = int(frac_train*len(pairs))\n",
    "train_set, val_set = pairs[:div], pairs[div:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2dbf78",
   "metadata": {},
   "source": [
    "## Step 2: Define model (2L decoder only transformer)\n",
    "Using NEEL NANDAS (INTERPRETABILITY) as inspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b1bf833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper class to get access to intermediate activations (inspired by Garcon)\n",
    "# It's a dummy module that is the identity function by default\n",
    "# I can wrap any intermediate activation in a HookPoint and get a convenient \n",
    "# way to add PyTorch hooks\n",
    "class HookPoint(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fwd_hooks = []\n",
    "        self.bwd_hooks = []\n",
    "    \n",
    "    def give_name(self, name):\n",
    "        # Called by the model at initialisation\n",
    "        self.name = name\n",
    "    \n",
    "    def add_hook(self, hook, dir='fwd'):\n",
    "        # Hook format is fn(activation, hook_name)\n",
    "        # Change it into PyTorch hook format (this includes input and output, \n",
    "        # which are the same for a HookPoint)\n",
    "        def full_hook(module, module_input, module_output):\n",
    "            return hook(module_output, name=self.name)\n",
    "        if dir=='fwd':\n",
    "            handle = self.register_forward_hook(full_hook)\n",
    "            self.fwd_hooks.append(handle)\n",
    "        elif dir=='bwd':\n",
    "            handle = self.register_backward_hook(full_hook)\n",
    "            self.bwd_hooks.append(handle)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid direction {dir}\")\n",
    "    \n",
    "    def remove_hooks(self, dir='fwd'):\n",
    "        if (dir=='fwd') or (dir=='both'):\n",
    "            for hook in self.fwd_hooks:\n",
    "                hook.remove()\n",
    "            self.fwd_hooks = []\n",
    "        if (dir=='bwd') or (dir=='both'):\n",
    "            for hook in self.bwd_hooks:\n",
    "                hook.remove()\n",
    "            self.bwd_hooks = []\n",
    "        if dir not in ['fwd', 'bwd', 'both']:\n",
    "            raise ValueError(f\"Invalid direction {dir}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e9ec45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture\n",
    "# I defined my own transformer from scratch so I'd fully understand each component \n",
    "# - I expect this wasn't necessary or particularly important, and a bunch of this \n",
    "# replicates existing PyTorch functionality\n",
    "\n",
    "# Embed & Unembed\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.einsum('dbp -> bpd', self.W_E[:, x])\n",
    "\n",
    "class Unembed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_U = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_vocab))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (x @ self.W_U)\n",
    "\n",
    "# Positional Embeddings\n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, max_ctx, d_model):\n",
    "        super().__init__()\n",
    "        self.W_pos = nn.Parameter(torch.randn(max_ctx, d_model)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x+self.W_pos[:x.shape[-2]]\n",
    "\n",
    "# LayerNorm\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, epsilon = 1e-4, model=[None]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.w_ln = nn.Parameter(torch.ones(d_model))\n",
    "        self.b_ln = nn.Parameter(torch.zeros(d_model))\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.model[0].use_ln:\n",
    "            x = x - x.mean(axis=-1)[..., None]\n",
    "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
    "            x = x * self.w_ln\n",
    "            x = x + self.b_ln\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_head, n_ctx, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_O = nn.Parameter(torch.randn(d_model, d_head * num_heads)/np.sqrt(d_model))\n",
    "        self.register_buffer('mask', torch.tril(torch.ones((n_ctx, n_ctx))))\n",
    "        self.d_head = d_head\n",
    "        self.hook_k = HookPoint()\n",
    "        self.hook_q = HookPoint()\n",
    "        self.hook_v = HookPoint()\n",
    "        self.hook_z = HookPoint()\n",
    "        self.hook_attn = HookPoint()\n",
    "        self.hook_attn_pre = HookPoint()\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.hook_k(torch.einsum('ihd,bpd->biph', self.W_K, x))\n",
    "        q = self.hook_q(torch.einsum('ihd,bpd->biph', self.W_Q, x))\n",
    "        v = self.hook_v(torch.einsum('ihd,bpd->biph', self.W_V, x))\n",
    "        attn_scores_pre = torch.einsum('biph,biqh->biqp', k, q)\n",
    "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (1 - self.mask[:x.shape[-2], :x.shape[-2]])\n",
    "        attn_matrix = self.hook_attn(F.softmax(self.hook_attn_pre(attn_scores_masked/np.sqrt(self.d_head)), dim=-1))\n",
    "        z = self.hook_z(torch.einsum('biph,biqp->biqh', v, attn_matrix))\n",
    "        z_flat = einops.rearrange(z, 'b i q h -> b q (i h)')\n",
    "        out = torch.einsum('df,bqf->bqd', self.W_O, z_flat)\n",
    "        return out\n",
    "\n",
    "# MLP Layers\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_model)/np.sqrt(d_model))\n",
    "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
    "        self.W_out = nn.Parameter(torch.randn(d_model, d_mlp)/np.sqrt(d_model))\n",
    "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
    "        self.act_type = act_type\n",
    "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
    "        self.hook_pre = HookPoint()\n",
    "        self.hook_post = HookPoint()\n",
    "        assert act_type in ['ReLU', 'GeLU']\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hook_pre(torch.einsum('md,bpd->bpm', self.W_in, x) + self.b_in)\n",
    "        if self.act_type=='ReLU':\n",
    "            x = F.relu(x)\n",
    "        elif self.act_type=='GeLU':\n",
    "            x = F.gelu(x)\n",
    "        x = self.hook_post(x)\n",
    "        x = torch.einsum('dm,bpm->bpd', self.W_out, x) + self.b_out\n",
    "        return x\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
    "        self.attn = Attention(d_model, num_heads, d_head, n_ctx, model=self.model)\n",
    "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
    "        self.mlp = MLP(d_model, d_mlp, act_type, model=self.model)\n",
    "        self.hook_attn_out = HookPoint()\n",
    "        self.hook_mlp_out = HookPoint()\n",
    "        self.hook_resid_pre = HookPoint()\n",
    "        self.hook_resid_mid = HookPoint()\n",
    "        self.hook_resid_post = HookPoint()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hook_resid_mid(x + self.hook_attn_out(self.attn((self.hook_resid_pre(x)))))\n",
    "        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n",
    "        return x\n",
    "\n",
    "# Full transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_vocab, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, use_cache=False, use_ln=True):\n",
    "        super().__init__()\n",
    "        self.cache = {}\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        self.embed = Embed(d_vocab, d_model)\n",
    "        self.pos_embed = PosEmbed(n_ctx, d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]) for i in range(num_layers)])\n",
    "        # self.ln = LayerNorm(d_model, model=[self])\n",
    "        self.unembed = Unembed(d_vocab, d_model)\n",
    "        self.use_ln = use_ln\n",
    "\n",
    "        for name, module in self.named_modules():\n",
    "            if type(module)==HookPoint:\n",
    "                module.give_name(name)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        # x = self.ln(x)\n",
    "        x = self.unembed(x)\n",
    "        return x\n",
    "\n",
    "    def set_use_cache(self, use_cache):\n",
    "        self.use_cache = use_cache\n",
    "    \n",
    "    def hook_points(self):\n",
    "        return [module for name, module in self.named_modules() if 'hook' in name]\n",
    "\n",
    "    def remove_all_hooks(self):\n",
    "        for hp in self.hook_points():\n",
    "            hp.remove_hooks('fwd')\n",
    "            hp.remove_hooks('bwd')\n",
    "    \n",
    "    def cache_all(self, cache, incl_bwd=False):\n",
    "        # Caches all activations wrapped in a HookPoint\n",
    "        def save_hook(tensor, name):\n",
    "            cache[name] = tensor.detach()\n",
    "        def save_hook_back(tensor, name):\n",
    "            cache[name+'_grad'] = tensor[0].detach()\n",
    "        for hp in self.hook_points():\n",
    "            hp.add_hook(save_hook, 'fwd')\n",
    "            if incl_bwd:\n",
    "                hp.add_hook(save_hook_back, 'bwd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940ab24e",
   "metadata": {},
   "source": [
    "## Step 3: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d42a0183",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "weight_decay = 1.0\n",
    "d_model = 128\n",
    "num_epochs = 10000\n",
    "save_models = True\n",
    "save_every = 100\n",
    "save_root = Path(\"phd/IMT4392/project/modulo/checkpoints\") if not local_host else Path(\"checkpoints\")\n",
    "stopping_thresh = -1\n",
    "seed = 0\n",
    "fn_name = 'add' #['add', 'subtract', 'x2xyy2','rand']\n",
    "\n",
    "# Model parameters\n",
    "num_layers = 1\n",
    "batch_style = 'full'\n",
    "d_vocab = p+1\n",
    "n_ctx = 3\n",
    "d_mlp = 4*d_model\n",
    "num_heads = 4\n",
    "assert d_model % num_heads == 0, \"Error with parameters\"\n",
    "d_head = d_model//num_heads\n",
    "act_type = 'ReLU' #@param ['ReLU', 'GeLU']\n",
    "use_ln = False\n",
    "random_answers = np.random.randint(low=0, high=p, size=(p, p))\n",
    "fns_dict = {'add': lambda x,y:(x+y)%p, 'subtract': lambda x,y:(x-y)%p, 'x2xyy2':lambda x,y:(x**2+x*y+y**2)%p, 'rand':lambda x,y:random_answers[x][y]}\n",
    "fn = fns_dict[fn_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac4116b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for model training\n",
    "\n",
    "def full_loss(model, data, device):\n",
    "    # Take the final position only\n",
    "    logits = model(data)[:, -1]\n",
    "    labels = torch.tensor([fn(i, j) for i, j, _ in data]).to(device)\n",
    "    return cross_entropy_high_precision(logits, labels)\n",
    "\n",
    "def cross_entropy_high_precision(logits, labels):\n",
    "    # Shapes: batch x vocab, batch\n",
    "    # Cast logits to float64 because log_softmax has a float32 underflow on overly \n",
    "    # confident data and can only return multiples of 1.2e-7 (the smallest float x\n",
    "    # such that 1+x is different from 1 in float32). This leads to loss spikes \n",
    "    # and dodgy gradients\n",
    "    logprobs = F.log_softmax(logits.to(torch.float64), dim=-1)\n",
    "    prediction_logprobs = torch.gather(logprobs, index=labels[:, None], dim=-1)\n",
    "    loss = -torch.mean(prediction_logprobs)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name add__2022_10_30__12_17_53\n",
      "0_1.5345_1.5357\n",
      "Saved model to checkpoints/add__2022_10_30__12_17_53/0.pth\n",
      "100_0.5767_2.0768\n",
      "Saved model to checkpoints/add__2022_10_30__12_17_53/100.pth\n",
      "200_-4.1928_2.7497\n",
      "Saved model to checkpoints/add__2022_10_30__12_17_53/200.pth\n",
      "300_-5.2439_2.7802\n",
      "Saved model to checkpoints/add__2022_10_30__12_17_53/300.pth\n",
      "400_-6.3731_2.8315\n",
      "Saved model to checkpoints/add__2022_10_30__12_17_53/400.pth\n",
      "500_-7.4775_2.8850\n",
      "Saved model to checkpoints/add__2022_10_30__12_17_53/500.pth\n",
      "600_-8.5674_2.9389\n",
      "Saved model to checkpoints/add__2022_10_30__12_17_53/600.pth\n",
      "700_-9.6416_2.9926\n",
      "Saved model to checkpoints/add__2022_10_30__12_17_53/700.pth\n",
      "800_-10.6992_3.0435\n",
      "Saved model to checkpoints/add__2022_10_30__12_17_53/800.pth\n",
      "900_-11.7297_3.0917\n",
      "Saved model to checkpoints/add__2022_10_30__12_17_53/900.pth\n",
      "1000_-12.7058_3.1348\n",
      "Saved model to checkpoints/add__2022_10_30__12_17_53/1000.pth\n"
     ]
    }
   ],
   "source": [
    "# Init model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Transformer(num_layers=num_layers, d_vocab=d_vocab, d_model=d_model, d_mlp=d_mlp, d_head=d_head, num_heads=num_heads, n_ctx=n_ctx, act_type=act_type, use_cache=False, use_ln=use_ln)\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizing process\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.98))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step/10, 1))\n",
    "\n",
    "# Run formalities\n",
    "run_name = f\"{fn_name}__\"+datetime.datetime.now().strftime(\"%Y_%m_%d__%H_%M_%S\")\n",
    "print(f'Run name {run_name}')\n",
    "\n",
    "if save_models:\n",
    "    os.mkdir(save_root/run_name)\n",
    "    save_dict = {'model':model.state_dict(), 'train_data':train_set, 'val_data':val_set}\n",
    "    torch.save(save_dict, save_root/run_name/'init.pth')\n",
    "    \n",
    "# Allocate lists for loss storage\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Train over several epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Calculate train and val loss\n",
    "    train_loss = full_loss(model, train_set, device)\n",
    "    train_losses.append(train_loss.item())\n",
    "    \n",
    "    val_loss = full_loss(model, val_set, device)\n",
    "    val_losses.append(val_loss.item())\n",
    "    \n",
    "    # Print status\n",
    "    if epoch%100 == 0: print(f\"{epoch}_{np.log(train_loss.item()):.4f}_{np.log(val_loss.item()):.4f}\")\n",
    "    \n",
    "    # Calc gradients and perform backprop\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if val_loss.item() < stopping_thresh:\n",
    "        break\n",
    "        \n",
    "    if (save_models) and (epoch%save_every == 0):\n",
    "        if val_loss.item() < stopping_thresh:\n",
    "            break\n",
    "        save_dict = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(save_dict, save_root/run_name/f\"{epoch}.pth\")\n",
    "        print(f\"Saved model to {save_root/run_name/f'{epoch}.pth'}\")\n",
    "        \n",
    "if not save_models:\n",
    "    os.mkdir(save_root/run_name)\n",
    "save_dict = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "    'train_loss': train_loss,\n",
    "    'val_loss': val_loss,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'epoch': epoch,\n",
    "}\n",
    "\n",
    "torch.save(save_dict, save_root/run_name/f\"final.pth\")\n",
    "print(f\"Saved model to {save_root/run_name/f'final.pth'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a3742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
